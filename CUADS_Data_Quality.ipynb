{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# CUADS Data Quality Notebook\n",
    "\n",
    "## About CUADS\n",
    "CUADS is a multimodal dataset contained ECG, PPG and GSR physiological signals recorded while participants viewed pre-tagged affective movie clips. After each movie clip, participants completed a SAM survey to evaluate arousal and valence.\n",
    "\n",
    "The dataset can obtained from IEEE DataPort here: [https://ieee-dataport.org/documents/clarkson-university-affective-dataset-cuads](https://ieee-dataport.org/documents/clarkson-university-affective-dataset-cuads)\n",
    "\n",
    "\n",
    "## About this notebook\n",
    "This notebook is used to perform quality assessment of the data contained in CUADS. We evaluate and score each of the three physiological signals, and the SAM survey response data.\n",
    "\n",
    "## Instructions\n",
    "Download and extract the CUADS dataset. In the first cell, update the value `dataset_root` to path to the `cuads` folder in the extracted dataset. This notebook produces several output images. The path to save the output can be set in the `dataset_output` variable, and defaults to `./output`.\n",
    "\n",
    "For help, please reach out to the authors."
   ]
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import matplotlib.pyplot as plt\n",
    "import neurokit2 as nk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from ardt.datasets import MultiDataset\n",
    "from ardt.datasets.cuads import CuadsDataset\n",
    "from ardt.datasets.cuads.CuadsDataset import CUADS_NUM_PARTICIPANTS, CUADS_NUM_MEDIA_FILES\n",
    "from ardt.datasets.ascertain import AscertainDataset\n",
    "from ardt.datasets.ascertain.AscertainDataset import ASCERTAIN_NUM_PARTICIPANTS, ASCERTAIN_NUM_MEDIA_FILES\n",
    "from ardt.datasets.dreamer import DreamerDataset\n",
    "from ardt.datasets.dreamer.DreamerDataset import DREAMER_NUM_PARTICIPANTS, DREAMER_NUM_MEDIA_FILES\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa, aggregate_raters\n",
    "\n",
    "# Path to the extracted dataset\n",
    "dataset_root='./dataset/cuads'\n",
    "\n",
    "# Path to write output images and numpy files\n",
    "dataset_output=f'./output'\n",
    "\n",
    "if not os.path.exists(dataset_root):\n",
    "    print(\"ERROR: No dataset found at {dataset_root}\")\n",
    "\n",
    "if not os.path.exists(dataset_output):\n",
    "    os.mkdir(dataset_output)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup Dataset for Processing\n",
    "\n",
    "Here we use ARDT to load the CUADS, ASCERTAIN and DREAMER datasets, combinbing them together into one large MultiDataset for analysis"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Fs=256                      # Physiological signal sampling rate. DO NOT MODIFY.\n",
    "CUADS_NUM_PARTICIPANTS=44   # CUADS enrolled 44 participants.\n",
    "CUADS_NUM_MOVIECLIPS=20     # Participants were each shown 20 movie clips\n",
    "\n",
    "datasets = []\n",
    "media_offset = 0\n",
    "participant_offset = 0\n",
    "\n",
    "print(\"Loading CUADS...\")\n",
    "cuads = CuadsDataset(mediafile_offset=participant_offset,participant_offset=media_offset)\n",
    "datasets.append(cuads)\n",
    "media_offset += CUADS_NUM_MEDIA_FILES\n",
    "participant_offset += CUADS_NUM_PARTICIPANTS\n",
    "\n",
    "print(\"Loading ASCERTAIN...\")\n",
    "ascertain = AscertainDataset(mediafile_offset=participant_offset,participant_offset=media_offset)\n",
    "datasets.append(ascertain)\n",
    "media_offset += ASCERTAIN_NUM_MEDIA_FILES\n",
    "participant_offset += ASCERTAIN_NUM_PARTICIPANTS\n",
    "\n",
    "print(\"Loading DREAMER...\")\n",
    "dreamer = DreamerDataset(mediafile_offset=participant_offset,participant_offset=media_offset)\n",
    "datasets.append(dreamer)\n",
    "media_offset += DREAMER_NUM_MEDIA_FILES\n",
    "participant_offset += DREAMER_NUM_PARTICIPANTS\n",
    "\n",
    "# Note -- calling preload and load_trials on the multiset calls it on each\n",
    "#         individual dataset too ... so you can still use dreamer, ascertain\n",
    "#         and cuads independently too...\n",
    "multiset = MultiDataset(datasets)   # 121 participants, 74 videos ...\n",
    "multiset.preload()\n",
    "multiset.load_trials()\n",
    "\n",
    "print(f\"MultiDataset has {len(multiset.participant_ids)} participants\")\n",
    "print(f\"MultiDataset has {len(multiset.media_ids)} media files\")\n",
    "print(f\"MultiDataset has {len(multiset.trials)} trials\")\n",
    "\n",
    "# `dataset` is used throughout this notebook ... pick any dataset individually, or\n",
    "# use the multiset...\n",
    "dataset = multiset\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ECG Scoring\n",
    "\n",
    "The following cell scores the ECGs using NeuroKit2. NeuroKit2 ontains several methods for cleaning an ECG, listed in the `methods` list. The Zhoe2018 method scores the cleaned ECG, rating them as `unacceptable`, `barely acceptable` or `excellent`\n",
    "\n",
    "For each ecg signal in the dataset, we attempt to find the NeuroKit2 method that results in the highest quality score.\n",
    "\n",
    "__NOTE:__ in the Data Descriptor submitted to IEEE-Data, we only report quality results using the `neurokit` Method. This method applies a 0.5 Hz high-pass butterworth filter (order = 5), followed by powerline filtering (nk defaults to 50 Hz, we specify 60Hz in the call to override). Below, we search all available methods and achieve \"Excellent\" scoring for all 714 samples in the dataset\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Map column numbers to ECG Channel Names - only used for logging\n",
    "# column_to_channel_name = {15: \"LA-RA\", 16: \"LL-LA\", 17: \"LL-RA\"}\n",
    " # Map Zhao2018 result labels to integer values\n",
    "quality_map = { \"Unacceptable\": 0, \"Unnacceptable\": 0, \"Barely acceptable\": 1, \"Excellent\": 2 }\n",
    "\n",
    "# List of methods for ECG Cleaning in NeuroKit2\n",
    "methods = [\"neurokit\", \"biosppy\", \"pantompkins1985\", \"hamilton2002\", \"elgendi2010\", \"engzeemod2012\", \"vg\"]\n",
    "\n",
    "# Used to track how many times each method resulted in the best score for each signal\n",
    "bestmethod_counts = {\"neurokit\": 0, \"biosppy\": 0, \"pantompkins1985\": 0, \"hamilton2002\": 0, \"elgendi2010\": 0, \"engzeemod2012\": 0, \"vg\": 0}\n",
    "\n",
    "# Used to track how many ecg signals received each score\n",
    "quality_results = { 0: 0, 1: 0, 2: 0 }\n",
    "\n",
    "# Used to track the best scores by ecg-channel\n",
    "quality_by_channel = {0: { 0: 0, 1: 0, 2: 0 }, 1: { 0: 0, 1: 0, 2: 0 }, 2: { 0: 0, 1: 0, 2: 0 } }\n",
    "\n",
    "for trial in dataset.trials:\n",
    "    signal = trial.load_signal_data('ECG')\n",
    "\n",
    "    for channel in range(trial.get_signal_metadata('ECG')['n_channels']):\n",
    "        quality = \"x\"\n",
    "        channel_dat=signal[channel+1,:]\n",
    "\n",
    "        methodidx = 0\n",
    "        best_quality = -1\n",
    "        best_quality_method = \"\"\n",
    "\n",
    "        # Loop until we get an \"Excellent\" result, or run out of methods to test...\n",
    "        while quality != \"Excellent\" and methodidx < len(methods):\n",
    "            current_method = methods[methodidx]\n",
    "\n",
    "            # Clean the ECG signal using the current method\n",
    "            ecg_cleaned = nk.ecg_clean(channel_dat, sampling_rate=Fs, powerline=60, method=current_method)\n",
    "\n",
    "            # Evaluate the ECG signal using fuzzy comprehensive SQI evaluation\n",
    "            quality = nk.ecg_quality(ecg_cleaned, sampling_rate=Fs, method='zhao2018', approach=\"fuzzy\")\n",
    "\n",
    "            # Is this better than the best one we've found so far?\n",
    "            if quality_map[quality] > best_quality:\n",
    "                best_quality = quality_map[quality]\n",
    "                best_quality_method = current_method\n",
    "\n",
    "            methodidx += 1\n",
    "\n",
    "        quality_by_channel[channel][best_quality] += 1\n",
    "        bestmethod_counts[best_quality_method] += 1\n",
    "        quality_results[best_quality] += 1\n",
    "\n",
    "print(\"\\nECG Signal Counts by Best Quality (0=Unacceptable, 1=Barely acceptable, 2=Excellent)\")\n",
    "print(quality_results)\n",
    "\n",
    "print(\"\\nECG Signal Counts by Best Method\")\n",
    "print(bestmethod_counts)\n",
    "\n",
    "print(\"\\nECG Signal Counts by By ECG-Channel (0=LA-RA, 1=LL-LA, 2=LL-RA)\")\n",
    "print(quality_by_channel)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PPG Scoring\n",
    "\n",
    "We score PPG by looking at the heartrate inferred from PPG, and computing the euclidean distance to the average heartrate computed across the 3 ECG channels. We then score them on their distance from ECG_HR based on the mean and stdev of the distances.\n",
    "\n",
    "If d < mean - stdev (between 0 and 1 stdev less tham mean), it is excellent\n",
    "if (mean-stdev) <= d <= (mean+stdev), it is good\n",
    "if d > (mean~stdev), it is poor\n",
    "\n",
    "```\n",
    "Let m be the mean distance\n",
    "Let s be the stdev of the distances\n",
    "\n",
    "  |----- EXCELLENT ----|--- GOOD ----|----- POOR -------|\n",
    "  |--------------------|------+------|------------------|\n",
    "min(d)               (m-s)    m    (m+s)              max(d)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hr_distances = []\n",
    "\n",
    "# NOTE: Only CUADS has PPG Data available so we score that directly instead of using the multiset\n",
    "\n",
    "for trial in cuads.trials:\n",
    "    print(f\"Participant {trial.participant_id} Video {trial.media_id}\")\n",
    "    ecg_hr_signal = trial.load_signal_data('ECGHR')[1:3,:].transpose()\n",
    "    ppg_hr_signal = trial.load_signal_data('PPGHR')[1,:].transpose()\n",
    "\n",
    "    ecg_avg_hr = np.array(ecg_hr_signal,dtype=float).mean(axis=1)\n",
    "    ppg_hr = np.array(ppg_hr_signal,dtype=float)\n",
    "    hr_distances.append(np.sqrt(np.sum(np.square(ecg_avg_hr-ppg_hr))))\n",
    "\n",
    "print(\"Heart Rate Stats:\")\n",
    "hr_distances = np.array(hr_distances)\n",
    "mu = np.mean(hr_distances)\n",
    "sigma = np.std(hr_distances)\n",
    "print(f\"Min: {np.min(hr_distances)})\")\n",
    "print(f\"Max: {np.max(hr_distances)}\")\n",
    "print(f\"Stdev: {sigma}\")\n",
    "print(f\"Mean: {mu}\")\n",
    "print(\"\\nScores:\")\n",
    "print(f\"Poor: {len([ d for d in hr_distances if d > (mu+sigma)])}\")\n",
    "print(f\"Good: {len([ d for d in hr_distances if (mu-sigma) <= d <= (mu+sigma)])}\")\n",
    "print(f\"Excellent: {len([ d for d in hr_distances if d < (mu-sigma)])}\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# GSR Scoring\n",
    "\n",
    "Implemented across the next two cells\n",
    "\n",
    "We score GSR based on RMSE of the Skin Conductance signals. We calculate a reference signal for each participant by averaging skin conductance for all samples from that participant. We then compute the distance from baseline for each signal and rate them as `poor`, `good` or `excellent` based on distance from the mean."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gsr_signals_by_participant = {}\n",
    "\n",
    "# Note - we only score GSR data from CUADS right now...\n",
    "\n",
    "for trial in cuads.trials:\n",
    "    if not trial.participant_id in gsr_signals_by_participant:\n",
    "        gsr_signals_by_participant[trial.participant_id] = []\n",
    "    print(f\"Participant {trial.participant_id} Video {trial.media_id}\")\n",
    "    skin_conductance = trial.load_signal_data('GSR')[1,:].transpose()\n",
    "    gsr_signals_by_participant[trial.participant_id].append(skin_conductance)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rmse_results = {}\n",
    "\n",
    "for participant, gsr_signals in gsr_signals_by_participant.items():\n",
    "    # Determine the maximum length of segmented data files for this participant\n",
    "    max_length = max(len(gsr_signal) for gsr_signal in gsr_signals)\n",
    "\n",
    "    # Pad all segmented data files to the maximum length with zeros\n",
    "    padded_datasets = [np.pad(gsr_signal, (0, max_length - len(gsr_signal)), mode='constant') for gsr_signal in gsr_signals]\n",
    "\n",
    "    # Compute the reference signal (mean across padded datasets)\n",
    "    reference_signal = np.mean(padded_datasets, axis=0)\n",
    "\n",
    "    # Compute normalized RMSE for each segmented data file\n",
    "    rmses = []\n",
    "    for gsr_signal in gsr_signals:\n",
    "        T_i = len(gsr_signal)  # Actual length of the dataset\n",
    "        padded_dataset = np.pad(gsr_signal, (0, max_length - T_i), mode='constant')  # Pad to match reference length\n",
    "        rmse = np.sqrt(np.sum((padded_dataset[:T_i] - reference_signal[:T_i]) ** 2) / T_i)  # Normalize by T_i\n",
    "        rmses.append(rmse)\n",
    "\n",
    "    # Compute thresholds for classification\n",
    "    mu = np.mean(rmses)  # Mean of RMSE values\n",
    "    sigma = np.std(rmses)  # Standard deviation of RMSE values\n",
    "\n",
    "    classifications = []\n",
    "    for rmse in rmses:\n",
    "        if rmse < mu - sigma:\n",
    "            classifications.append(\"Excellent\")\n",
    "        elif mu - sigma <= rmse <= mu + sigma:\n",
    "            classifications.append(\"Good\")\n",
    "        else:\n",
    "            classifications.append(\"Poor\")\n",
    "\n",
    "    # Store results and classifications for the participant\n",
    "    rmse_results[participant] = {\n",
    "        \"RMSE Values\": rmses,\n",
    "        \"Classifications\": classifications\n",
    "    }\n",
    "\n",
    "# Convert results into a DataFrame for easier analysis\n",
    "participant_results = []\n",
    "for participant, results in rmse_results.items():\n",
    "    for i, (rmse, classification) in enumerate(zip(results[\"RMSE Values\"], results[\"Classifications\"])):\n",
    "        participant_results.append({\n",
    "            \"Participant\": participant,\n",
    "            \"Segmented File ID\": i + 1,\n",
    "            \"RMSE\": rmse,\n",
    "            \"Classification\": classification\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(participant_results)\n",
    "\n",
    "# Display the results in tabular format\n",
    "# Count the total number of each classification\n",
    "classification_counts = results_df[\"Classification\"].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(\"Total Classifications:\")\n",
    "print(classification_counts)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Participant Response Evaluation\n",
    "\n",
    "We consider Randolph's Kappa to assess intra-rater agreement of all participants for each movie clip. We expect better than chance agreement and hope to fair-to-moderate agreement based on other studies.\n",
    "\n",
    "We also consider how participants' classification of each movie clip matches the expected classification label (where movies are classified into Quadrants 1, 2, 3 or 4 on the Valence-Arousal space). Confusion matrices are generated to visualize this information."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# These are what we expect based on pretagged media from MANHOB-HCI\n",
    "###################################################################\n",
    "participant_responses_quadrant = np.zeros(shape=(len(dataset.participant_ids), len(dataset.media_ids)), dtype=int)\n",
    "expected_responses_quadrant = np.zeros(shape=(len(dataset.media_ids)), dtype=int)\n",
    "\n",
    "for trial in dataset.trials:\n",
    "    participant_responses_quadrant[trial.participant_id-1][trial.media_id-1] \\\n",
    "        = trial.load_ground_truth()\n",
    "\n",
    "    expected_responses_quadrant[trial.media_id-1] = trial.expected_response\n",
    "\n",
    "def convert_quadrant_to_arousal(value):\n",
    "    if value == 0:\n",
    "        return 0\n",
    "    elif value in [1, 2]:\n",
    "        return 1\n",
    "    elif value in [3, 4]:\n",
    "        return -1\n",
    "\n",
    "def convert_quadrant_to_valence(value):\n",
    "    if value == 0:\n",
    "        return 0\n",
    "    elif value in [1, 4]:\n",
    "        return 1\n",
    "    elif value in [2, 3]:\n",
    "        return -1\n",
    "\n",
    "\n",
    "# Apply the conversion\n",
    "participant_responses_arousal = np.vectorize(convert_quadrant_to_arousal)(participant_responses_quadrant)\n",
    "participant_responses_valence = np.vectorize(convert_quadrant_to_valence)(participant_responses_quadrant)\n",
    "expected_responses_arousal = np.vectorize(convert_quadrant_to_arousal)(expected_responses_quadrant)\n",
    "expected_responses_valence = np.vectorize(convert_quadrant_to_valence)(expected_responses_quadrant)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_kappa_scores( kappa_input, title ):\n",
    "    kappa_scores = []\n",
    "    for i in range(kappa_input.shape[0]):\n",
    "        clip_input = kappa_input[i:i + 1, :]  # Extract one row for the current clip\n",
    "        if np.sum(clip_input) > 0:  # Ensure there is at least one valid response\n",
    "            kappa_score = fleiss_kappa(clip_input, method='randolph')\n",
    "            kappa_scores.append(kappa_score)\n",
    "        else:\n",
    "            kappa_scores.append(None)  # Handle empty clips gracefully\n",
    "\n",
    "    ## Create bar chart for Fleiss' Kappa scores\n",
    "    clip_indices = list(range(1, len(kappa_scores) + 1))\n",
    "    data = [score if score is not None else 0 for score in kappa_scores]\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.bar(clip_indices, data, color='skyblue')\n",
    "    plt.xticks(clip_indices)\n",
    "    plt.xlabel(\"Movie Clip\")\n",
    "    plt.ylabel(\"Randolph's Kappa Score\")\n",
    "    plt.title(title)\n",
    "    plt.ylim(np.min(kappa_scores)*-1.1, 1.1*np.max(np.abs(kappa_scores)))  # Fleiss' Kappa ranges from -1 to 1\n",
    "    plt.axhline(0, color='gray', linestyle='--', linewidth=0.8)  # Reference line for 0 agreement\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def prepare_kappa_input( responses ):\n",
    "    (result, labels) = aggregate_raters(responses.transpose())\n",
    "\n",
    "    zeroidx = np.where(labels == 0)[0]\n",
    "    result = np.delete(result, zeroidx, axis=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "def pad_kappa_inputs_to_n_raters( kappa_input ):\n",
    "    n_raters = int(kappa_input.sum(axis=1).max())\n",
    "    if not np.all(kappa_input.sum(axis=1) == n_raters):\n",
    "        # Normalize all clips to have the same number of ratings\n",
    "        for i in range(kappa_input.shape[0]):\n",
    "            total_responses = int(kappa_input[i].sum())\n",
    "\n",
    "            if total_responses < n_raters:\n",
    "                # Add missing responses proportionally based on existing counts\n",
    "                proportions = kappa_input[i] / total_responses\n",
    "                missing_responses = n_raters - total_responses\n",
    "                additional_responses = np.random.multinomial(missing_responses, proportions)\n",
    "\n",
    "                kappa_input[i] += additional_responses\n",
    "\n",
    "    return kappa_input\n",
    "\n",
    "def process_kappa( participant_responses, response_type ):\n",
    "    # Prepare kappa input based on arousal results\n",
    "    kappa_input = prepare_kappa_input(participant_responses)\n",
    "    kappa_input = pad_kappa_inputs_to_n_raters(kappa_input)\n",
    "    plot_kappa_scores(kappa_input, f\"Randolph's Kappa Agreement Scores Across Movie Clips - {response_type}\")\n",
    "\n",
    "    # Calculate Fleiss' Kappa using only non-neutral movies\n",
    "    kappa_score = fleiss_kappa(kappa_input, method='randolph')\n",
    "    print(\"Randolph's Kappa Score:\", kappa_score)\n",
    "\n",
    "    # Identify indices of non-neutral movies\n",
    "    # Filter the fleiss_input matrix to exclude neutral movies\n",
    "    # non_neutral_movie_indices = [i for i, label in enumerate(expected_responses_quadrant) if label != 0]\n",
    "    # filtered_kappa_input = kappa_input[non_neutral_movie_indices]\n",
    "    # plot_kappa_scores(filtered_kappa_input, f\"Randolph's Kappa Agreement Scores Across Non-Neutral Movie Clips - {response_type}\")\n",
    "    #\n",
    "    # # Calculate Randolph's Kappa using only non-neutral movies\n",
    "    # filtered_kappa_score = fleiss_kappa(filtered_kappa_input, method='randolph')\n",
    "    # print(\"Randolph's Kappa Score (excluding neutral movies):\", filtered_kappa_score)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "process_kappa( participant_responses_arousal, \"Arousal\")\n",
    "process_kappa( participant_responses_valence, \"Valence\")\n",
    "process_kappa(participant_responses_quadrant, \"Quadrant\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def stats(y_true, y_pred, classes):\n",
    "    # Compute Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Compute Precision and Recall for Each Class\n",
    "    precision = precision_score(y_true, y_pred, average=None)  # Per-class precision\n",
    "    recall = recall_score(y_true, y_pred, average=None)        # Per-class recall\n",
    "\n",
    "    # Macro and Weighted Averages\n",
    "    macro_precision = precision_score(y_true, y_pred, average='macro')\n",
    "    macro_recall = recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    weighted_precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    weighted_recall = recall_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    # Print Results\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Per-Class Precision:\", precision)\n",
    "    print(\"Per-Class Recall:\", recall)\n",
    "    print(f\"Macro Precision: {macro_precision:.2f}\")\n",
    "    print(f\"Macro Recall: {macro_recall:.2f}\")\n",
    "    print(f\"Weighted Precision: {weighted_precision:.2f}\")\n",
    "    print(f\"Weighted Recall: {weighted_recall:.2f}\")\n",
    "\n",
    "    # Optional: Full Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=classes))\n",
    "\n",
    "\n",
    "def make_matrix(cm_responses, cm_labels, cm_classes, cm_xlabel, cm_ylabel, fname):\n",
    "    non_neutral_movie_indices = [i for i, label in enumerate(cm_labels) if label != 0]\n",
    "    confusion_responses = cm_responses[:, non_neutral_movie_indices]\n",
    "\n",
    "    expected_labels = cm_labels[non_neutral_movie_indices]\n",
    "\n",
    "    valid_responses = confusion_responses.flatten() != 0\n",
    "    y_true = np.tile(expected_labels, confusion_responses.shape[0])[valid_responses]\n",
    "    y_pred = confusion_responses.flatten()[valid_responses]\n",
    "    stats(y_true, y_pred, cm_classes)\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        normalize=\"pred\"\n",
    "    )\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f', cmap=\"Greys\", xticklabels=cm_classes, yticklabels=cm_classes, annot_kws={\"size\": 18})\n",
    "    plt.xlabel(cm_xlabel, fontsize=12)\n",
    "    plt.ylabel(cm_ylabel, fontsize=12)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.savefig(fname, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "cm_responses = participant_responses_quadrant\n",
    "cm_labels = expected_responses_quadrant\n",
    "cm_classes = [\"I\", \"II\", \"III\", \"IV\" ]\n",
    "cm_xlabel = \"Expected Rating (Quadrant)\"\n",
    "cm_ylabel = \"Participant Rating (Quadrant)\"\n",
    "make_matrix(\n",
    "    cm_responses,\n",
    "    cm_labels,\n",
    "    cm_classes,\n",
    "    cm_xlabel,\n",
    "    cm_ylabel,\n",
    "    f\"{dataset_output}/quadrant_matrix.png\"\n",
    ")\n",
    "\n",
    "cm_responses = participant_responses_arousal\n",
    "cm_labels = expected_responses_arousal\n",
    "cm_classes = [\"Low\", \"High\" ]\n",
    "cm_xlabel = \"Expected Rating (Arousal)\"\n",
    "cm_ylabel = \"Participant Rating (Arousal)\"\n",
    "make_matrix(\n",
    "    cm_responses,\n",
    "    cm_labels,\n",
    "    cm_classes,\n",
    "    cm_xlabel,\n",
    "    cm_ylabel,\n",
    "    f\"{dataset_output}/arousal_matrix.png\"\n",
    ")\n",
    "\n",
    "cm_responses = participant_responses_valence\n",
    "cm_labels = expected_responses_valence\n",
    "cm_classes = [\"Negative\", \"Positive\" ]\n",
    "cm_xlabel = \"Expected Rating (Valence)\"\n",
    "cm_ylabel = \"Participant Rating (Valence)\"\n",
    "make_matrix(\n",
    "    cm_responses,\n",
    "    cm_labels,\n",
    "    cm_classes,\n",
    "    cm_xlabel,\n",
    "    cm_ylabel,\n",
    "    f\"{dataset_output}/valence_matrix.png\"\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
